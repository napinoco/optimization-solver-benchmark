name: Optimization Solver Benchmark

on:
  # Run on push to main branch
  push:
    branches: [ main ]
  
  # Run on pull requests to main
  pull_request:
    branches: [ main ]
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      solvers:
        description: 'Comma-separated list of solvers to run (e.g., scipy,cvxpy)'
        required: false
        default: 'scipy,cvxpy'
        type: string
      problem_set:
        description: 'Problem set to use for benchmarks'
        required: false
        default: 'light_set'
        type: choice
        options:
          - light_set
          - medium_set
          - large_set
      timeout:
        description: 'Solver timeout in seconds (10-3600)'
        required: false
        default: '300'
        type: string
      skip_cross_platform:
        description: 'Skip cross-platform testing to save CI minutes'
        required: false
        default: true
        type: boolean
      verbose_logging:
        description: 'Enable verbose logging for debugging'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.12'

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  benchmark:
    runs-on: ubuntu-22.04
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        cache-dependency-path: |
          requirements/base.txt
          requirements/python.txt
        
    - name: Create virtual environment
      run: |
        python -m venv venv
        source venv/bin/activate
        echo "VIRTUAL_ENV=${VIRTUAL_ENV}" >> $GITHUB_ENV
        echo "${VIRTUAL_ENV}/bin" >> $GITHUB_PATH
        
    - name: Install dependencies
      run: |
        source venv/bin/activate
        pip install --upgrade pip
        pip install -r requirements/base.txt
        pip install -r requirements/python.txt
        
    - name: Verify installation
      run: |
        source venv/bin/activate
        python --version
        pip list
        
    - name: Validate environment
      run: |
        source venv/bin/activate
        python main.py --validate
        
    - name: Validate input parameters
      run: |
        source venv/bin/activate
        # Determine which solvers to run
        if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
          SOLVERS="${{ github.event.inputs.solvers }}"
          PROBLEM_SET="${{ github.event.inputs.problem_set }}"
          TIMEOUT="${{ github.event.inputs.timeout }}"
          VERBOSE="${{ github.event.inputs.verbose_logging }}"
        else
          SOLVERS="scipy,cvxpy"
          PROBLEM_SET="light_set"
          TIMEOUT="300"
          VERBOSE="false"
        fi
        
        echo "## Input Parameters" >> $GITHUB_STEP_SUMMARY
        echo "- **Trigger**: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Solvers**: $SOLVERS" >> $GITHUB_STEP_SUMMARY
        echo "- **Problem Set**: $PROBLEM_SET" >> $GITHUB_STEP_SUMMARY
        echo "- **Timeout**: ${TIMEOUT}s" >> $GITHUB_STEP_SUMMARY
        echo "- **Verbose Logging**: $VERBOSE" >> $GITHUB_STEP_SUMMARY
        echo "- **Skip Cross-Platform**: ${{ github.event.inputs.skip_cross_platform || 'false' }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Validate timeout is numeric and reasonable
        if ! [[ "$TIMEOUT" =~ ^[0-9]+$ ]] || [ "$TIMEOUT" -lt 10 ] || [ "$TIMEOUT" -gt 3600 ]; then
          echo "❌ Invalid timeout: $TIMEOUT (must be 10-3600 seconds)" >> $GITHUB_STEP_SUMMARY
          exit 1
        fi
        
        # Export for next step
        echo "BENCHMARK_SOLVERS=$SOLVERS" >> $GITHUB_ENV
        echo "BENCHMARK_PROBLEM_SET=$PROBLEM_SET" >> $GITHUB_ENV
        echo "BENCHMARK_TIMEOUT=$TIMEOUT" >> $GITHUB_ENV
        echo "BENCHMARK_VERBOSE=$VERBOSE" >> $GITHUB_ENV
        
    - name: Run benchmarks
      run: |
        source venv/bin/activate
        
        echo "Running benchmarks with:"
        echo "  Solvers: $BENCHMARK_SOLVERS"
        echo "  Problem set: $BENCHMARK_PROBLEM_SET"
        echo "  Timeout: $BENCHMARK_TIMEOUT"
        echo "  Verbose: $BENCHMARK_VERBOSE"
        
        # Update timeout in config if needed
        if [ "$BENCHMARK_TIMEOUT" != "300" ]; then
          sed -i "s/timeout: [0-9]*/timeout: $BENCHMARK_TIMEOUT/" config/benchmark_config.yaml
        fi
        
        # Build command with optional verbose flag
        BENCHMARK_CMD="python main.py --benchmark --solvers \"$BENCHMARK_SOLVERS\" --problem-set \"$BENCHMARK_PROBLEM_SET\""
        if [ "$BENCHMARK_VERBOSE" = "true" ]; then
          BENCHMARK_CMD="$BENCHMARK_CMD --verbose"
        fi
        
        # Run the benchmark
        eval $BENCHMARK_CMD
        
    - name: Generate reports
      run: |
        source venv/bin/activate
        python main.py --report
        
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results
        path: |
          database/results.db
          logs/benchmark.log
        retention-days: 30
        
    - name: Upload generated reports
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-reports
        path: docs/
        retention-days: 30
        
    - name: Display benchmark summary
      run: |
        source venv/bin/activate
        echo "## Benchmark Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Extract summary from logs
        if [ -f logs/benchmark.log ]; then
          echo "### Results" >> $GITHUB_STEP_SUMMARY
          echo "```" >> $GITHUB_STEP_SUMMARY
          grep -E "(Success rate|Total results|Benchmark completed)" logs/benchmark.log | tail -10 >> $GITHUB_STEP_SUMMARY
          echo "```" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Show artifacts
        echo "### Generated Artifacts" >> $GITHUB_STEP_SUMMARY
        echo "- **benchmark-results**: Database and logs" >> $GITHUB_STEP_SUMMARY
        echo "- **benchmark-reports**: HTML reports" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Show environment info with detailed Ubuntu version
        echo "### Environment" >> $GITHUB_STEP_SUMMARY
        UBUNTU_VERSION=$(cat /etc/os-release | grep PRETTY_NAME | cut -d'"' -f2)
        echo "- **OS**: $UBUNTU_VERSION (ubuntu-latest resolved)" >> $GITHUB_STEP_SUMMARY
        echo "- **Kernel**: $(uname -r)" >> $GITHUB_STEP_SUMMARY
        echo "- **Python**: ${{ env.PYTHON_VERSION }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Solvers**: $(echo "${{ github.event.inputs.solvers || 'scipy,cvxpy' }}")" >> $GITHUB_STEP_SUMMARY
        
    - name: Check for validation errors
      run: |
        source venv/bin/activate
        # Check if there were any validation errors in the logs
        if grep -q "Validation errors" logs/benchmark.log; then
          echo "⚠️ Validation errors detected in benchmark results!" >> $GITHUB_STEP_SUMMARY
          echo "### Validation Errors" >> $GITHUB_STEP_SUMMARY
          echo "```" >> $GITHUB_STEP_SUMMARY
          grep "Validation errors" logs/benchmark.log >> $GITHUB_STEP_SUMMARY
          echo "```" >> $GITHUB_STEP_SUMMARY
        fi
        
        if grep -q "Validation warnings" logs/benchmark.log; then
          echo "### Validation Warnings" >> $GITHUB_STEP_SUMMARY
          echo "```" >> $GITHUB_STEP_SUMMARY
          grep "Validation warnings" logs/benchmark.log >> $GITHUB_STEP_SUMMARY
          echo "```" >> $GITHUB_STEP_SUMMARY
        fi
        
    - name: Setup Pages
      if: github.ref == 'refs/heads/main' && github.event_name != 'pull_request'
      uses: actions/configure-pages@v4
      continue-on-error: true
      id: pages-setup
      
    - name: Upload pages artifact
      if: github.ref == 'refs/heads/main' && github.event_name != 'pull_request' && steps.pages-setup.outcome == 'success'
      uses: actions/upload-pages-artifact@v3
      with:
        path: ./docs
        
    - name: Deploy to GitHub Pages
      if: github.ref == 'refs/heads/main' && github.event_name != 'pull_request' && steps.pages-setup.outcome == 'success'
      id: deployment
      uses: actions/deploy-pages@v4
      
    - name: Pages deployment status
      if: github.ref == 'refs/heads/main' && github.event_name != 'pull_request'
      run: |
        if [ "${{ steps.pages-setup.outcome }}" = "success" ]; then
          echo "✅ Reports deployed to GitHub Pages successfully!" >> $GITHUB_STEP_SUMMARY
          echo "📄 View at: https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/" >> $GITHUB_STEP_SUMMARY
        else
          echo "⚠️ GitHub Pages deployment skipped" >> $GITHUB_STEP_SUMMARY
          echo "To enable Pages deployment:" >> $GITHUB_STEP_SUMMARY
          echo "1. Go to repository Settings → Pages" >> $GITHUB_STEP_SUMMARY
          echo "2. Select 'GitHub Actions' as source" >> $GITHUB_STEP_SUMMARY
          echo "3. Re-run this workflow" >> $GITHUB_STEP_SUMMARY
        fi

  # Job to run on different OS for broader compatibility testing
  # DISABLED BY DEFAULT to speed up CI runtime - only runs when manually triggered and explicitly enabled
  benchmark-cross-platform:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-22.04, macos-latest, windows-latest]
        python-version: ['3.10', '3.11', '3.12']
      fail-fast: false
      
    # Only run cross-platform testing when explicitly requested (skip_cross_platform defaults to true)
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.skip_cross_platform != 'true'
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
        cache-dependency-path: |
          requirements/base.txt
          requirements/python.txt
        
    - name: Create virtual environment (Unix)
      if: runner.os != 'Windows'
      run: |
        python -m venv venv
        source venv/bin/activate
        echo "VIRTUAL_ENV=${VIRTUAL_ENV}" >> $GITHUB_ENV
        echo "${VIRTUAL_ENV}/bin" >> $GITHUB_PATH
        
    - name: Create virtual environment (Windows)
      if: runner.os == 'Windows'
      run: |
        python -m venv venv
        venv\Scripts\activate
        echo "VIRTUAL_ENV=$env:VIRTUAL_ENV" >> $env:GITHUB_ENV
        echo "$env:VIRTUAL_ENV\Scripts" >> $env:GITHUB_PATH
        
    - name: Install dependencies (Unix)
      if: runner.os != 'Windows'
      run: |
        source venv/bin/activate
        pip install --upgrade pip
        pip install -r requirements/base.txt
        pip install -r requirements/python.txt
        
    - name: Install dependencies (Windows)
      if: runner.os == 'Windows'
      run: |
        venv\Scripts\activate
        pip install --upgrade pip
        pip install -r requirements/base.txt
        pip install -r requirements/python.txt
        
    - name: Run quick benchmark (Unix)
      if: runner.os != 'Windows'
      run: |
        source venv/bin/activate
        python main.py --benchmark --solvers scipy --problem-set light_set
        
    - name: Run quick benchmark (Windows)
      if: runner.os == 'Windows'
      run: |
        venv\Scripts\activate
        python main.py --benchmark --solvers scipy --problem-set light_set
        
    - name: Upload cross-platform results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ matrix.os }}-py${{ matrix.python-version }}
        path: |
          database/results.db
          logs/benchmark.log
        retention-days: 7